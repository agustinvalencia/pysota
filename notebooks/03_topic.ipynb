{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f078efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pysota.core import Publication\n",
    "from pysota.process import Persistence\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# db: list[Publication] = Persistence.load_files(path=Path('../results/clustered/euclidean'), query_name='cluster_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ad302",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'robots'\n",
    "word2 = 'weapons'\n",
    "word3 = 'dog'\n",
    "\n",
    "vector = nlp(word1.lower())[0].vector + nlp(word2.lower())[0].vector # + nlp(word3.lower())[0].vector\n",
    "res = nlp.vocab.vectors.most_similar(vector.reshape(1, -1))\n",
    "nlp.vocab[res[0][0][0]].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acdd5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Sample documents\n",
    "# documents = [doc.abstract for doc in db]\n",
    "\n",
    "# # Convert documents into a document-term matrix\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "# dtm = vectorizer.fit_transform(documents)\n",
    "\n",
    "# # Set the number of topics\n",
    "# n_topics = 10\n",
    "\n",
    "# # Initialize and fit the LDA model\n",
    "# lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "# lda.fit(dtm)\n",
    "\n",
    "# # Display the top words for each topic\n",
    "# n_top_words = 3\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# for topic_idx, topic in enumerate(lda.components_):\n",
    "#     print(f\"Topic {topic_idx}:\")\n",
    "#     top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "#     print(\" \".join(top_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\n",
    "    \"representation\", \n",
    "    \"learning\", \n",
    "    \"learn\", \n",
    "    \"training\", \n",
    "    \"train\", \n",
    "    \"supervision\",\n",
    "    \"supervised\",\n",
    "    \"supervise\",\n",
    "    \"method\", \n",
    "    \"model\", \n",
    "    \"datum\", \n",
    "    \"self\" ,\n",
    "    \"task\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, exclude=exclude):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha and token.lemma_ not in exclude and token.lemma_ != 'ADV'\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Preprocess each document\n",
    "# texts = [preprocess(doc.abstract) for doc in db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fa479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(dictionary, num_topics=10, passes=100):\n",
    "    # Create a dictionary and corpus\n",
    "    # dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    # Filter out words that occur less than 2 documents, or more than 50% of the documents\n",
    "    # dictionary.filter_extremes(no_below=2)\n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "    return lda_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4605856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(lda_model, texts, dictionary):\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score: {coherence_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_topics_to_dataframe(lda_model, num_words=5):\n",
    "    # Extract topics from the LDA model\n",
    "    topics = lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False)\n",
    "    \n",
    "    # Initialize a list to hold the parsed data\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over each topic\n",
    "    for topic_num, terms in topics:\n",
    "        for term, weight in terms:\n",
    "            data.append([topic_num, term, weight])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Topic', 'Term', 'Weight'])\n",
    "    \n",
    "    # Sort the DataFrame by Topic and Weight in descending order\n",
    "    df = df.sort_values(by=['Topic', 'Weight'], ascending=[True, False]).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have an LdaModel object named 'lda_model'\n",
    "# df_topics = lda_topics_to_dataframe(lda_model, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(10): \n",
    "    db: list[Publication] = Persistence.load_files(path=Path('../results/clustered/euclidean'), query_name=f'cluster_{cluster}')\n",
    "\n",
    "    msg = f\"Cluster {cluster} has {len(db)} documents\"\n",
    "    print(f\"\\n{msg}\")\n",
    "    print('-' * len(msg))\n",
    "\n",
    "    texts = [preprocess(doc.abstract) for doc in db]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    lda_model = train_lda(dictionary, num_topics=5, passes=100)\n",
    "    coherence(lda_model, texts, dictionary)\n",
    "    df_topics = lda_topics_to_dataframe(lda_model, num_words=5)\n",
    "\n",
    "    for topic in df_topics.Topic.unique():\n",
    "        terms = df_topics[df_topics.Topic == topic].Term.tolist()\n",
    "        print(f\"Topic {topic}: {', '.join(terms)} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
