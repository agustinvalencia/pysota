{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pysota.core import Publication\n",
    "from pysota.process import Persistence\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ffae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\n",
    "    \"representation\", \n",
    "    \"learning\", \n",
    "    \"learn\", \n",
    "    \"training\", \n",
    "    \"train\", \n",
    "    \"supervision\",\n",
    "    \"supervised\",\n",
    "    \"supervise\",\n",
    "    \"method\", \n",
    "    \"model\", \n",
    "    \"datum\", \n",
    "    \"self\" ,\n",
    "    \"task\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, exclude=exclude):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha and token.lemma_ not in exclude and token.lemma_ != 'ADV'\n",
    "    ]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db: list[Publication] = Persistence.load_files(path=Path('../results/clean'), query_name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [preprocess(i.abstract) for i in db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=3, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All steps together\n",
    "model = BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics, probs = model.fit_transform(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10feb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in model.get_topic_info().Representation:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3cb125",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
