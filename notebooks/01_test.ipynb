{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.8015959858894348\n",
      "salty fries <-> hamburgers 0.5733411312103271\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger package!\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_1 =\"'<jats:p>The global demographic shift towards an aging population has intensified the prevalence of diseases like osteoporosis, characterized by fragile bones and heightened fracture risk, particularly in the spine, hips, and wrists. This condition, more common in women, results from low bone mass and poor structure, leading to weakened bones and increased susceptibility to fractures. While Dual Energy X-ray Absorptiometry (DEXA) has been a traditional diagnostic tool, its limited availability, cost, and radiation exposure pose challenges. However, Computer- Aided Diagnosis (CAD) has elevated diagnostic accuracy.  In modern medical education, Deep Learning, Machine Learning, and Artificial Intelligence have revolutionized healthcare. These technologies enable precise osteoporosis diagnosis by analyzing clinical data and images using Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). By combining advanced algorithms with medical expertise, these systems offer automated detection and diagnosis, improving early intervention and reducing osteoporosis's impact on individuals and healthcare systems. This integration underscores the critical role of technology in healthcare advancement.        Keywords: AI, deep learning, feature extraction, ML, osteoporosis</jats:p>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_2 = \"<jats:p>Despite widespread skepticism of data analytics and artificial intelligence (AI) in adjudication, the Social Security Administration (SSA) pioneered path-breaking AI tools that became embedded in multiple levels of its adjudicatory process. How did this happen? What lessons can we draw from the SSA experience for AI in government? We first discuss how early strategic investments by the SSA in data infrastructure, policy, and personnel laid the groundwork for AI. Second, we document how SSA overcame a wide range of organizational barriers to develop some of the most advanced use cases in adjudication. Third, we spell out important lessons for AI innovation and governance in the public sector. We highlight the importance of leadership to overcome organizational barriers, “blended expertise” spanning technical and domain knowledge, operational data, early piloting, and continuous evaluation. AI should not be conceived of as a one-off IT product, but rather as part of continuous improvement. AI governance is quality assurance.</jats:p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_3 = \"The processing pipeline consists of one or more pipeline components that are called on the Doc in order. The tokenizer runs before the components. Pipeline components can be added using Language.add_pipe. They can contain a statistical model and trained weights, or only make rule-based modifications to the Doc. spaCy provides a range of built-in components for different language processing tasks and also allows adding custom components.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9537680745124817\n",
      "0.9474533200263977\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(abstract_1)\n",
    "doc2 = nlp(abstract_2)\n",
    "doc3 = nlp(abstract_3)\n",
    "print(doc1.similarity(doc2))\n",
    "print(doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "  - '<jats:p>The global demographic shift towards an aging population has intensified the prevalence of diseases like osteoporosis, characterized by fragile bones and heightened fracture risk, particularly in the spine, hips, and wrists. This condition, more common in women, results from low bone mass and poor structure, leading to weakened bones and increased susceptibility to fractures. While Dual Energy X-ray Absorptiometry (DEXA) has been a traditional diagnostic tool, its limited availability, cost, and radiation exposure pose challenges. However, Computer- Aided Diagnosis (CAD) has elevated diagnostic accuracy.  In modern medical education, Deep Learning, Machine Learning, and Artificial Intelligence have revolutionized healthcare. These technologies enable precise osteoporosis diagnosis by analyzing clinical data and images using Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). By combining advanced algorithms with medical expertise, these systems offer automated detection and diagnosis, improving early intervention and reducing osteoporosis's impact on individuals and healthcare systems. This integration underscores the critical role of technology in healthcare advancement.        Keywords: AI, deep learning, feature extraction, ML, osteoporosis</jats:p>\n",
      "  - <jats:p>Despite widespread skepticism of data analytics and artificial intelligence (AI) in adjudication, the Social Security Administration (SSA) pioneered path-breaking AI tools that became embedded in multiple levels of its adjudicatory process. How did this happen? What lessons can we draw from the SSA experience for AI in government? We first discuss how early strategic investments by the SSA in data infrastructure, policy, and personnel laid the groundwork for AI. Second, we document how SSA overcame a wide range of organizational barriers to develop some of the most advanced use cases in adjudication. Third, we spell out important lessons for AI innovation and governance in the public sector. We highlight the importance of leadership to overcome organizational barriers, “blended expertise” spanning technical and domain knowledge, operational data, early piloting, and continuous evaluation. AI should not be conceived of as a one-off IT product, but rather as part of continuous improvement. AI governance is quality assurance.</jats:p\n",
      "  - The processing pipeline consists of one or more pipeline components that are called on the Doc in order. The tokenizer runs before the components. Pipeline components can be added using Language.add_pipe. They can contain a statistical model and trained weights, or only make rule-based modifications to the Doc. spaCy provides a range of built-in components for different language processing tasks and also allows adding custom components.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "documents = [abstract_1, abstract_2, abstract_3]\n",
    "doc_vectors = [nlp(doc).vector for doc in documents]\n",
    "\n",
    "X = np.array(doc_vectors)\n",
    "\n",
    "dbscan = DBSCAN(eps=2.9, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Group documents by their cluster labels\n",
    "clusters = {}\n",
    "for idx, label in enumerate(dbscan.labels_):\n",
    "    clusters.setdefault(label, []).append(documents[idx])\n",
    "\n",
    "# Print the clusters; note that label -1 indicates noise points\n",
    "for cluster_id, docs in clusters.items():\n",
    "    if cluster_id == -1:\n",
    "        print(\"Noise:\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "    for doc in docs:\n",
    "        print(f\"  - {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "  - I love cats and dogs.\n",
      "  - Dogs are great companions.\n",
      "Cluster 1:\n",
      "  - The sky is blue and beautiful.\n",
      "  - The weather is bright and sunny.\n",
      "Cluster 2:\n",
      "  - I enjoy reading about science.\n",
      "  - Science and technology are fascinating subjects.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Load a spaCy model with word vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Sample text documents\n",
    "documents = [\n",
    "    \"I love cats and dogs.\",\n",
    "    \"Dogs are great companions.\",\n",
    "    \"The sky is blue and beautiful.\",\n",
    "    \"The weather is bright and sunny.\",\n",
    "    \"I enjoy reading about science.\",\n",
    "    \"Science and technology are fascinating subjects.\"\n",
    "]\n",
    "\n",
    "# Process documents and extract their vector representations\n",
    "doc_vectors = [nlp(doc).vector for doc in documents]\n",
    "X = np.array(doc_vectors)\n",
    "\n",
    "# Configure DBSCAN clustering\n",
    "# eps: maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "# min_samples: the number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "dbscan = DBSCAN(eps=2.9, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Group documents by their cluster labels\n",
    "clusters = {}\n",
    "for idx, label in enumerate(dbscan.labels_):\n",
    "    clusters.setdefault(label, []).append(documents[idx])\n",
    "\n",
    "# Print the clusters; note that label -1 indicates noise points\n",
    "for cluster_id, docs in clusters.items():\n",
    "    if cluster_id == -1:\n",
    "        print(\"Noise:\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "    for doc in docs:\n",
    "        print(f\"  - {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'Hello,\\n\\n\\tWorld!  This is a\\ttest: 123.\\n\\n'\n",
      "Cleaned text: 'Hello World This is a test 123'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_string(input_string):\n",
    "    # Remove all non-word characters except spaces\n",
    "    cleaned_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
    "    # Replace multiple consecutive whitespace characters with a single space\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string)\n",
    "    # Strip leading and trailing whitespace\n",
    "    cleaned_string = cleaned_string.strip()\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Hello,\\n\\n\\tWorld!  This is a\\ttest: 123.\\n\\n\"\n",
    "    cleaned_text = clean_string(sample_text)\n",
    "    print(\"Original text:\", repr(sample_text))\n",
    "    print(\"Cleaned text:\", repr(cleaned_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
